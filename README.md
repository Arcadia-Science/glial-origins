# Comparing gene expression across species based on protein structure instead of sequence
We investigated protein structure predictions as an alternative to protein sequence homology for comparing cell types and gene expression across species based on single-cell RNA-seq data.

---

## Purpose of this repository
This repository presents our explorations into developing a computational framework for generating shared feature groups between genes in multiple species.  
We examined the impact of using sequence similarity groups (Orthogroups generated by [OrthoFinder](https://github.com/davidemms/OrthoFinder)) or structural similarity groups (Structural clusters generated by [FoldSeek](https://github.com/steineggerlab/foldseek) clustering of [AlphaFold structures](https://alphafold.ebi.ac.uk/)) on cross-species gene expression analysis.

We focused on three datasets of adult brain single-cell RNA-Sequencing generated by the Guo lab at Zhejiang University using the Microwell-Seq platform:
- sample ["Brain8"](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM3768152) from the [Jiang et al. 2021](https://www.frontiersin.org/articles/10.3389/fcell.2021.743421/full) zebrafish cell atlas
- sample ["Brain1"](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM2906405) from the [Han et al. 2018](https://www.sciencedirect.com/science/article/pii/S0092867418301168#sec4) mouse cell atlas
- sample ["Xenopus_brain_COL65"](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM6214268) from the [Liao et al. 2022](https://www.nature.com/articles/s41467-022-31949-2) Xenopus laevis adult cell atlas

Our results are summarized in a [Pub](https://arcadia-research.pubpub.org/pub/idea-cell-type-evolution) on our website.

---

## Data exploration
If you'd simply like to explore the feature space embeddings we generated in this manuscript, we've built a Binder environment where you can quickly download compressed versions of our data from Zenodo and analyze our data using scanpy. TODO

---

# Reproducing our analysis
If you'd like to run our pipelines and analyses for this repository, you'll need the following:
- a Unix machine with at **least 32GB of RAM** that is able to build a conda environment specified by [glial_origins_tidy.yml](env/glial_origins_tidy.yml)
- at least **250GB of free disk space** to store the files generated by the analysis
- an **Amazon Web Services S3 bucket** where you can upload and store your files
    - you should use `awscli` or `boto3` to set up permissions for the bucket on your local machine
- a **clone of this GitHub repository**
    - you should modify the [install_locs.py](env/install_locs.py) file to reflect the file paths to your local installs of the listed software
    - you should also change the `S3_BUCKET_ADDRESS` variable to the address of your S3 bucket, following the format of the example provided

---

## Directory structure

### Files:
- this [README.md](README.md) file containing description of directories and their functions
- [.gitignore](.gitignore) file containing patterns such as `output/` and `.ipynb_checkpoints/` to be ignored by git
- MIT [LICENSE](LICENSE) file 

### Directories:
- `docs/` folder containing documents for mkdocs
- `env/` folder containing conda install `.yml` files
- `utils/` general scripts, tools, and functions for all processes
- `notebooks/` Jupyter notebooks describing the exploratory analyses and linking together functions
  - `1_downloading/`: notebooks that download expression, genome FASTA and annotation, and cell type annotation data
  - `2_feature-embedding/`: notebooks that aggregate gene expression into Orthogroup or Structural cluster abundance matrices
  - `3_single-species-exploration/`: notebooks that visualize gene expression vs. abundance for each species in Orthogroup or Structural cluster space
  - `4_multi-species-exploration/`: notebooks that visualize feature abundance for all three species in Orthogroup or Structural cluster space
- `output/`  folder for files generated by analyses; .gitignore this directory
  - files and directories in this folder are auto-generated by `biofile_handling`
  
---

## Setup

The following instructions would allow you to reproduce the results of our analyses using an AWS Cloud9 instance.

### On a fresh AWS Cloud9 Instance

1. Create a new AWS Cloud9 instance.  
- Give it a useful name (e.g. `glial-origins-analysis`)
- Choose a configuration that gives you ~64GB of RAM (e.g. m4.4xlarge).  
- Use a Amazon Linux 2 machine.

2. After starting up the instance, git clone this repository using SSH and authenticate.  
On the command line, use:  
`git clone git@github.com:Arcadia-Science/glial-origins.git`. 

3. Move into the GitHub repo.  
`cd glial-origins/`

4. Resize the local volume to enable more storage.  
Set the number of GB of storage you want to have as an integer (e.g. 250GB below).  
The resize script is from [this tutorial](https://docs.aws.amazon.com/cloud9/latest/user-guide/move-environment.html#move-environment-resize).   
`bash env/resize.sh 100`
> You probably want at least 250GB for a base analysis.  
> Up to 1000GB is not unreasonable if you're running a variety of analyses. 

5. Download and install Miniconda.  
On the command line, use:  
`curl -JLO https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh`  
`bash Miniconda3-latest-Linux-x86_64.sh`  
> Accept all defaults.

6. Start Conda.  
On the command line, use:  
`source ~/.bashrc`

7. Set Conda base channels.  
On the command line, use:  
`conda config --add channels defaults`  
`conda config --add channels bioconda`  
`conda config --add channels conda-forge`  

8. Install Mamba.  
On the command line, use:  
`conda install mamba`
> Choose `y` when prompted.

9. Create base environment.  
`mamba env create -f env/glial_origins_tidy.yml`  
> This will take some time, perhaps ~5-10min.

10. Activate the new conda environment.  
`conda activate glial_origins_tidy`
> Note: You'll also need to run this command whenever you restart the Cloud9 instance.

11. Run Jupyter.  
`jupyter lab --ip 0.0.0.0 --port 8888`
> This should output a bunch of lines of text.  
> You'll want to pay attention to the lines that say something like:  
` To access the server, open this file in a browser:  
        file:///home/ec2-user/.local/share/jupyter/runtime/jpserver-30829-open.html 
    Or copy and paste one of these URLs:  
        http://ip-172-31-22-55.us-west-1.compute.internal:8888/lab?token=dbaec68e69abc5a9e6ccc5bf413eb2a170c938c37ddb55c1  
     or http://127.0.0.1:8888/lab?token=dbaec68e69abc5a9e6ccc5bf413eb2a170c938c37ddb55c1`  
> You'll use the long string after `token=` to set the passphrase for the Jupyter lab server in step 13.  

12. Go to the Cloud9 console and go to the EC2 instance.  
- From the Cloud9 "Your Environments" section, click on the name of your new environment (e.g. `glial-origins-analysis`).
- Under "EC2 Instance", click on "Go To Instance."
- Select the instance in the checkbox menu.
- On the bottom half of the page, click the "Security" tab.
- Click on the hyperlink under "Security Groups."
- Click the "Edit Inbound Rules" button at the top-right of the bottom table.
- Click the "Add Rule" button.
- Under "Port Range" type `8888`.
- Under "Source" choose `Anywhere - IPv4`.
- Click the orange "Save rules" button.

13. Login to the Jupyter server.
- From the EC2 Instance checkbox menu, select the instance.
- Copy the "Public IPv4 DNS" value.
- Paste into a new web browser tab and add `:8888` to the end of the address.
- Copy and paste the string after `token=` in step 11 into the `Token` box.
- Type a secure password into the `New Password` box.
- Click the `Log in and set new password` button.

14. Start working on analyses!

### On an existing Cloud9 Instance

1. After starting up the instance, git clone this repository using HTTPS and authenticate.  
On the command line, use:  
`git clone https://github.com/Arcadia-Science/glial-origins.git`  

2. If Conda or Mamba are not already installed, install them following steps 4-7 above.

3. Create and start the base environment using steps 8-10 above.

4. Run Jupyter lab and set up the server using steps 11-13 above. If you have already set security group rules, you may not need to do this again.

5. Start working on analyses!

---

## Pipeline organization

### General naming conventions
Notebooks for running analyses are prefixed with a number based on their function, as described below.

`1_` – notebooks for downloading genome sequences, annotations, and genes x cells matrices  
    `a_` – notebooks for downloading cell type annotation data  
`2_` – notebooks for performing embedding of datasets  
`3_` – notebooks for analyzing cells of a single species and comparing to the same cells in a new embedding  
    `c_` – notebooks for generating plots of gene expression vs. feature abundance  
`4_` – notebooks for analyzing cells of multiple species in the same joint embedding  
    `b_` – notebooks for generating interactive plots of joint embedding spaces using Plotly  

## Pipeline organization

### General naming conventions
Notebooks for running analyses are prefixed with a number based on their function, as described below.

`1_` – notebooks for downloading genome sequences, annotations, and genes x cells matrices  
    `a_` – notebooks for downloading cell type annotation data  
`2_` – notebooks for performing embedding of datasets  
`3_` – notebooks for analyzing cells of a single species and comparing to the same cells in a new embedding  
    `c_` – notebooks for generating plots of gene expression vs. feature abundance  
`4_` – notebooks for analyzing cells of multiple species in the same joint embedding  
    `b_` – notebooks for generating interactive plots of joint embedding spaces using Plotly  

### Vertebrate adult brain analysis (Drer, Mmus, Xlae) using **OrthoFinder** and **AlphaFold → FoldSeek**
Analysis of cells from three adult brain scRNA-Seq datasets from zebrafish, mouse, and frog (_Xenopus laevis_) using an Orthogroup or Structural scluster embedding.  
Our analysis can be reproduced by running the provided notebooks in the following order.  

0. Make sure there's an empty `output/` folder in the home `glial-origins/` directory.  
> This should automatically be created when you first run a script.

1. Download necessary genome FASTA, genome annotation GFF, and genes x cells matrix TXT files:
- [1_downloading/1_Drer_adultbrain_downloading.ipynb](notebooks/1_downloading/1_Drer_adultbrain_downloading.ipynb)
- [1_downloading/1_Mmus_adultbrain_downloading.ipynb](notebooks/1_downloading/1_Mmus_adultbrain_downloading.ipynb)
- [1_downloading/1_Xlae_adultbrain_downloading.ipynb](notebooks/1_downloading/1_Xlae_adultbrain_downloading.ipynb)
> This downloads the three datasets and generates necessary files for the next steps in the analysis.

2. Download and format cell type annotation files:
- [1_downloading/a_Drer_adultbrain_cellannot_downloading.ipynb](notebooks/1_downloading/a_Drer_adultbrain_cellannot_downloading.ipynb)
- [1_downloading/a_Mmus_adultbrain_cellannot_downloading.ipynb](notebooks/1_downloading/a_Mmus_adultbrain_cellannot_downloading.ipynb)
- [1_downloading/a_Xlae_adultbrain_cellannot_downloading.ipynb](notebooks/1_downloading/a_Xlae_adultbrain_cellannot_downloading.ipynb)
> This downloads cell type annotations for three datasets and generates necessary files for the next steps in the analysis.

3. Run the OrthoFinder and FoldSeek analyses:
- [2_feature-embedding/2_DrerMmusXlae_adultbrain_runOrthoFinder.ipynb](notebooks/2_feature-embedding/2_DrerMmusXlae_adultbrain_runOrthoFinder.ipynb)
> This runs OrthoFinder on peptides from the three datasets.  

- [2_feature-embedding/d_OrthoFinder_renamer.ipynb](notebooks/2_feature-embedding/d_OrthoFinder_renamer.ipynb)  
> This generates and updated OrthoFinder .tsv where protein IDs are replaced by gene names, for easier interpretation.  

- [2_feature-embedding/2_DrerMmusXlae_adultbrain_runFoldSeek.ipynb](notebooks/2_feature-embedding/2_DrerMmusXlae_adultbrain_runFoldSeek.ipynb)
> This downloads AlphaFold structures for all available proteins for each species, then clusters structures using FoldSeek.

4. Visualize cells from each species and collect differentially expressed Orthogroups/ Structural clusters:
- [3_single-species-exploration/3_Drer_adultbrain_exploration-FoldSeek.ipynb](notebooks/3_single-species-exploration/3_Drer_adultbrain_exploration-FoldSeek.ipynb)
- [3_single-species-exploration/3_Drer_adultbrain_exploration-OrthoFinder.ipynb](notebooks/3_single-species-exploration/3_Drer_adultbrain_exploration-OrthoFinder.ipynb)
- [3_single-species-exploration/3_Mmus_adultbrain_exploration-FoldSeek.ipynb](notebooks/3_single-species-exploration/3_Mmus_adultbrain_exploration-FoldSeek.ipynb)
- [3_single-species-exploration/3_Mmus_adultbrain_exploration-OrthoFinder.ipynb](notebooks/3_single-species-exploration/3_Mmus_adultbrain_exploration-OrthoFinder.ipynb)
- [3_single-species-exploration/3_Xlae_adultbrain_exploration-FoldSeek.ipynb](notebooks/3_single-species-exploration/3_Xlae_adultbrain_exploration-FoldSeek.ipynb)
- [3_single-species-exploration/3_Xlae_adultbrain_exploration-OrthoFinder.ipynb](notebooks/3_single-species-exploration/3_Xlae_adultbrain_exploration-OrthoFinder.ipynb)
> These notebooks visualize and analyze gene expression/ feature abundance for each specices for each embedding space.  
> The notebooks generate plots for visualization purposes and also generate lists of differentially expressed Orthogroups or Structural clusters for the next step.  

- [3_single-species-exploration/c_Drer_adultbrain_supplementary_plots.ipynb](notebooks/3_single-species-exploration/c_Drer_adultbrain_supplementary_plots.ipynb)
> This notebook generates plots of gene expression vs. feature abundance for select Zebrafish genes.  
> The notebook should be easily modifiable to show similar plots for the other species in the analysis.  

5. Visualize cells from all three species in joint embedding spaces:
- [4_multi-species-exploration/4_DrerMmusXlae_adultbrain_exploration-Foldseek.ipynb](notebooks/4_multi-species-exploration/4_DrerMmusXlae_adultbrain_exploration-Foldseek.ipynb)
- [4_multi-species-exploration/4_DrerMmusXlae_adultbrain_exploration-OrthoFinder.ipynb](notebooks/4_multi-species-exploration/4_DrerMmusXlae_adultbrain_exploration-OrthoFinder.ipynb)
> These notebooks visualize and analyze feature abundance for all three species in either Orthogroup or Structural cluster feature space.  

- [4_multi-species-exploration/4_DrerMmusXlae_adultbrain_exploration-Foldseek_genesets.ipynb](notebooks/4_multi-species-exploration/4_DrerMmusXlae_adultbrain_exploration-Foldseek_genesets.ipynb)
- [4_multi-species-exploration/4_DrerMmusXlae_adultbrain_exploration-OrthoFinder_genesets.ipynb](notebooks/4_multi-species-exploration/4_DrerMmusXlae_adultbrain_exploration-OrthoFinder_genesets.ipynb)
> These notebooks perform a parameter sweep of top DE features for both OG and SC feature space.  

- [4_multi-species-exploration/b_DrerMmusXlae_FoldSeek_plotly-testing.ipynb](notebooks/4_multi-species-exploration/b_DrerMmusXlae_FoldSeek_plotly-testing.ipynb)
- [4_multi-species-exploration/b_DrerMmusXlae_FoldSeek_plotly-testing.ipynb](notebooks/4_multi-species-exploration/b_DrerMmusXlae_FoldSeek_plotly-testing.ipynb)
> These notebooks build Plotly interactive HTML plots for each of the two embedding spaces.  

---

## About **`biofile_handling`**
To get started, check out the documentation at the [GitHub Pages website](https://arcadia-science.github.io/glial-origins/).

### BioFile objects
Files in this project are handled using the BioFile class. See image below for a summary.

![BioFile Basics](https://user-images.githubusercontent.com/6531296/206793321-537f9254-0c73-48b3-9c6d-949d4aa44799.png)

#### What are BioFile objects used for?

BioFile objects are used to track the myriad metadata associated with a given type of file. 
For example, a file might be associated with a specific species and a specific tissue type.
A file might have been downloaded from a particular URL and might live at a specific address on Amazon S3.
All of these attributes are tracked by a BioFile.

#### What can I do with BioFile objects?

BioFile objects hold custom attributes depending on the specific type of file.
For example, a `GenomeFastaFile` object might have an associated `version`.
These attributes can themselves be a BioFile object; for example, a `GenomeGtfFile` might point to the correct `GenomeFastaFile` it is associated with.

Some BioFile objects also have class-specific functions (referred to as "methods" in Python).
For example, the `GenomeGffFile` class has a method `.to_gtf()` which converts the GFF file to GTF format, preserving a link to related the `GenomeFastaFle`.

#### How can I access BioFile attributes?

BioFile attibutes can be accessed using a period operator, as shown in the image below.

![BioFile Attributes](https://user-images.githubusercontent.com/6531296/192902111-792a70f0-f2fc-4993-9fc1-e74f8106bf70.png)

BioFile objects that store links to other BioFile objects can allow for hierarchical access.
For example, you could quickly get the filename of the `GenomeFastaFile` associated with a `GenomeGtfFile` object by using `GenomeGtfFile.GenomeFastaFile.filename`.

### BioFileDockets

BioFileDockets are another class of objects used in this analysis.
BioFileDocket objects store all BioFile objects associated with a specific dataset.

![Docket Objects](https://user-images.githubusercontent.com/6531296/206793358-7ef91d7b-e17a-4c7d-85ac-3958ebfef4c1.png)

#### What are BioFileDocket objects used for?

BioFileDocket objects keep track of all BioFile objects associated with a specific dataset.
For example, a mouse adult brain RNA-Seq dataset might have a `BioFileDocket` that stores the `GenomeFastaFile`, `GenomeGtfFile`, `IdmmFile`, `GxcFile`, and other objects specific to that dataset.

BioFileDocket objects are particularly useful for collecting the most important BioFile objects generated by each Python script or Jupyter notebook and passing these variables between scripts.

To do this, we use the `dill` package to create a "pickle" of the BioFileDocket variable.
The "pickle" is a file ending in `.pkl`, which can be loaded into a different script to keep the exact same information stored in the original BioFileDocket.

Pickling BioFileDockets is particularly helpful for the analysis pipelines in this project, in which we have to perform analyses on diverse datatypes and species and pass files between individual Python scripts.
Some of the analyses we do only work with data in one species, while others require data from multiple species.
Pickled Docket objects allow for standardized storage of information between different scripts, as well as a means of tracking the history of changes to many different files associated with each analysis.

#### How can I access BioFileDocket attributes?

BioFileDocket objects store attributes in the same way as BioFile objects. For example, for a mouse adult brain RNA-Seq dataset, you might get a filename for the genes by cells matrix using `Mmus_BioFileDocket.gxc.filename`.

---

## Contributing

See how we recognize [feedback and contributions to our code](https://github.com/Arcadia-Science/arcadia-software-handbook/blob/main/guides-and-standards/guide-credit-for-contributions.md).
